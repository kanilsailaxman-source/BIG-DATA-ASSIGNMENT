{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f239158-e281-40f5-b97e-d2346de8d1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>hotel_id</th><th>booking_id</th><th>location</th><th>booking_date</th><th>stay_duration</th><th>amount_spent</th><th>status</th><th>rating</th><th>hotel_name</th><th>city</th><th>customer_name</th><th>city</th></tr></thead><tbody><tr><td>121</td><td>215</td><td>1</td><td>Hyderabad</td><td>2025-01-15</td><td>3</td><td>21500.0</td><td>Booked</td><td>5</td><td>Park Hyatt</td><td>Hyderabad</td><td>Rina Kapoor</td><td>Hyderabad</td></tr><tr><td>142</td><td>207</td><td>2</td><td>Mumbai</td><td>2025-03-22</td><td>2</td><td>19800.0</td><td>Booked</td><td>4</td><td>Four Seasons</td><td>Mumbai</td><td>Kareena Kapoor</td><td>Kolkata</td></tr><tr><td>183</td><td>210</td><td>3</td><td>Bangalore</td><td>2025-08-10</td><td>5</td><td>35000.0</td><td>Cancelled</td><td>0</td><td>ITC Gardenia</td><td>Bangalore</td><td>Pankaj Tripathi</td><td>Bangalore</td></tr><tr><td>104</td><td>213</td><td>4</td><td>Hyderabad</td><td>2025-07-04</td><td>4</td><td>45000.0</td><td>Booked</td><td>5</td><td>Taj Falaknuma Palace</td><td>Hyderabad</td><td>Rohan Mehra</td><td>Hyderabad</td></tr><tr><td>165</td><td>211</td><td>5</td><td>Bangalore</td><td>2025-02-18</td><td>1</td><td>8500.0</td><td>Booked</td><td>3</td><td>Shangri-La</td><td>Bangalore</td><td>Kajol Devgan</td><td>Bangalore</td></tr><tr><td>196</td><td>219</td><td>6</td><td>Chennai</td><td>2025-06-30</td><td>3</td><td>24000.0</td><td>Booked</td><td>4</td><td>Taj Coromandel</td><td>Chennai</td><td>Zaira Wasim</td><td>Kolkata</td></tr><tr><td>117</td><td>209</td><td>7</td><td>Bangalore</td><td>2025-09-01</td><td>2</td><td>22000.0</td><td>Booked</td><td>5</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Priya Sharma</td><td>Bangalore</td></tr><tr><td>138</td><td>206</td><td>8</td><td>Mumbai</td><td>2025-04-12</td><td>7</td><td>65000.0</td><td>Cancelled</td><td>0</td><td>Trident Nariman Point</td><td>Mumbai</td><td>Rani Mukerji</td><td>Mumbai</td></tr><tr><td>159</td><td>204</td><td>9</td><td>Delhi</td><td>2025-05-05</td><td>3</td><td>18000.0</td><td>Booked</td><td>4</td><td>ITC Maurya</td><td>Delhi</td><td>Sridevi Kapoor</td><td>Bangalore</td></tr><tr><td>170</td><td>218</td><td>10</td><td>Chennai</td><td>2025-08-25</td><td>4</td><td>29000.0</td><td>Booked</td><td>4</td><td>The Leela Palace</td><td>Chennai</td><td>Raveena Tandon</td><td>Chennai</td></tr><tr><td>101</td><td>217</td><td>11</td><td>Chennai</td><td>2025-01-20</td><td>2</td><td>16000.0</td><td>Booked</td><td>5</td><td>ITC Grand Chola</td><td>Chennai</td><td>Anjali Patil</td><td>Chennai</td></tr><tr><td>122</td><td>205</td><td>12</td><td>Mumbai</td><td>2025-03-14</td><td>3</td><td>32000.0</td><td>Booked</td><td>5</td><td>The Taj Mahal Palace</td><td>Mumbai</td><td>Sameer Ali</td><td>Chennai</td></tr><tr><td>143</td><td>202</td><td>13</td><td>Delhi</td><td>2025-07-19</td><td>1</td><td>9500.0</td><td>Booked</td><td>3</td><td>Taj Palace</td><td>Delhi</td><td>Saif Ali Khan</td><td>Delhi</td></tr><tr><td>164</td><td>212</td><td>14</td><td>Bangalore</td><td>2025-02-28</td><td>5</td><td>41000.0</td><td>Cancelled</td><td>0</td><td>Conrad Bengaluru</td><td>Bangalore</td><td>Govinda Ahuja</td><td>Chennai</td></tr><tr><td>185</td><td>216</td><td>15</td><td>Hyderabad</td><td>2025-06-09</td><td>2</td><td>17500.0</td><td>Booked</td><td>4</td><td>The Westin Mindspace</td><td>Hyderabad</td><td>Rajkummar Rao</td><td>Delhi</td></tr><tr><td>106</td><td>209</td><td>16</td><td>Bangalore</td><td>2025-09-03</td><td>3</td><td>28000.0</td><td>Booked</td><td>5</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Amit Kumar</td><td>Bangalore</td></tr><tr><td>127</td><td>214</td><td>17</td><td>Hyderabad</td><td>2025-04-21</td><td>4</td><td>33000.0</td><td>Booked</td><td>4</td><td>ITC Kohenur</td><td>Hyderabad</td><td>Swati Pandit</td><td>Hyderabad</td></tr><tr><td>148</td><td>208</td><td>18</td><td>Mumbai</td><td>2025-08-16</td><td>2</td><td>24000.0</td><td>Booked</td><td>5</td><td>St. Regis</td><td>Mumbai</td><td>Alia Bhatt</td><td>Kolkata</td></tr><tr><td>169</td><td>220</td><td>19</td><td>Chennai</td><td>2025-01-11</td><td>6</td><td>39000.0</td><td>Cancelled</td><td>0</td><td>Hyatt Regency</td><td>Chennai</td><td>Suniel Shetty</td><td>Hyderabad</td></tr><tr><td>190</td><td>203</td><td>20</td><td>Delhi</td><td>2025-05-30</td><td>3</td><td>21000.0</td><td>Booked</td><td>4</td><td>Leela Palace</td><td>Delhi</td><td>Vidya Balan</td><td>Kolkata</td></tr><tr><td>111</td><td>210</td><td>21</td><td>Bangalore</td><td>2025-07-27</td><td>2</td><td>19000.0</td><td>Booked</td><td>4</td><td>ITC Gardenia</td><td>Bangalore</td><td>Kavita Nair</td><td>Bangalore</td></tr><tr><td>132</td><td>206</td><td>22</td><td>Mumbai</td><td>2025-02-04</td><td>4</td><td>36000.0</td><td>Booked</td><td>5</td><td>Trident Nariman Point</td><td>Mumbai</td><td>Nitin Saxena</td><td>Mumbai</td></tr><tr><td>153</td><td>201</td><td>23</td><td>Delhi</td><td>2025-06-18</td><td>1</td><td>11000.0</td><td>Booked</td><td>4</td><td>The Oberoi</td><td>Delhi</td><td>Aamir Khan</td><td>Bangalore</td></tr><tr><td>174</td><td>219</td><td>24</td><td>Chennai</td><td>2025-09-02</td><td>3</td><td>25500.0</td><td>Cancelled</td><td>0</td><td>Taj Coromandel</td><td>Chennai</td><td>Karisma Kapoor</td><td>Mumbai</td></tr><tr><td>195</td><td>215</td><td>25</td><td>Hyderabad</td><td>2025-03-09</td><td>5</td><td>38000.0</td><td>Booked</td><td>5</td><td>Park Hyatt</td><td>Hyderabad</td><td>Fatima Sana Shaikh</td><td>Bangalore</td></tr><tr><td>116</td><td>218</td><td>26</td><td>Chennai</td><td>2025-08-07</td><td>2</td><td>17000.0</td><td>Booked</td><td>3</td><td>The Leela Palace</td><td>Chennai</td><td>Aditya Narayan</td><td>Chennai</td></tr><tr><td>137</td><td>204</td><td>27</td><td>Delhi</td><td>2025-01-25</td><td>4</td><td>26000.0</td><td>Booked</td><td>4</td><td>ITC Maurya</td><td>Delhi</td><td>Vivek Oberoi</td><td>Delhi</td></tr><tr><td>158</td><td>207</td><td>28</td><td>Mumbai</td><td>2025-04-03</td><td>3</td><td>28000.0</td><td>Booked</td><td>5</td><td>Four Seasons</td><td>Mumbai</td><td>Dharmendra Deol</td><td>Chennai</td></tr><tr><td>179</td><td>211</td><td>29</td><td>Bangalore</td><td>2025-07-13</td><td>2</td><td>18500.0</td><td>Cancelled</td><td>0</td><td>Shangri-La</td><td>Bangalore</td><td>Om Puri</td><td>Delhi</td></tr><tr><td>199</td><td>213</td><td>30</td><td>Hyderabad</td><td>2025-02-14</td><td>6</td><td>62000.0</td><td>Booked</td><td>5</td><td>Taj Falaknuma Palace</td><td>Hyderabad</td><td>Sara Ali Khan</td><td>Hyderabad</td></tr><tr><td>120</td><td>205</td><td>31</td><td>Mumbai</td><td>2025-05-23</td><td>2</td><td>23000.0</td><td>Booked</td><td>4</td><td>The Taj Mahal Palace</td><td>Mumbai</td><td>Anil Mehta</td><td>Mumbai</td></tr><tr><td>141</td><td>209</td><td>32</td><td>Bangalore</td><td>2025-01-08</td><td>4</td><td>37000.0</td><td>Booked</td><td>5</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Hrithik Roshan</td><td>Bangalore</td></tr><tr><td>162</td><td>217</td><td>33</td><td>Chennai</td><td>2025-08-31</td><td>3</td><td>23000.0</td><td>Booked</td><td>4</td><td>ITC Grand Chola</td><td>Chennai</td><td>Sanjay Dutt</td><td>Mumbai</td></tr><tr><td>182</td><td>214</td><td>34</td><td>Hyderabad</td><td>2025-03-28</td><td>1</td><td>12000.0</td><td>Cancelled</td><td>0</td><td>ITC Kohenur</td><td>Hyderabad</td><td>Nawazuddin Siddiqui</td><td>Chennai</td></tr><tr><td>103</td><td>208</td><td>35</td><td>Mumbai</td><td>2025-06-25</td><td>5</td><td>55000.0</td><td>Booked</td><td>5</td><td>St. Regis</td><td>Mumbai</td><td>Sarika Desai</td><td>Mumbai</td></tr><tr><td>124</td><td>202</td><td>36</td><td>Delhi</td><td>2025-09-04</td><td>2</td><td>19000.0</td><td>Booked</td><td>4</td><td>Taj Palace</td><td>Delhi</td><td>Harish Chandra</td><td>Kolkata</td></tr><tr><td>145</td><td>212</td><td>37</td><td>Bangalore</td><td>2025-04-17</td><td>3</td><td>26500.0</td><td>Booked</td><td>4</td><td>Conrad Bengaluru</td><td>Bangalore</td><td>Ranveer Singh</td><td>Hyderabad</td></tr><tr><td>166</td><td>220</td><td>38</td><td>Chennai</td><td>2025-07-08</td><td>4</td><td>28000.0</td><td>Booked</td><td>3</td><td>Hyatt Regency</td><td>Chennai</td><td>Ajay Devgan</td><td>Kolkata</td></tr><tr><td>186</td><td>216</td><td>39</td><td>Hyderabad</td><td>2025-02-22</td><td>2</td><td>18000.0</td><td>Cancelled</td><td>0</td><td>The Westin Mindspace</td><td>Hyderabad</td><td>Ayushmann Khurrana</td><td>Mumbai</td></tr><tr><td>107</td><td>203</td><td>40</td><td>Delhi</td><td>2025-05-11</td><td>5</td><td>34000.0</td><td>Booked</td><td>5</td><td>Leela Palace</td><td>Delhi</td><td>Sneha Shah</td><td>Delhi</td></tr><tr><td>128</td><td>219</td><td>41</td><td>Chennai</td><td>2025-01-30</td><td>3</td><td>22500.0</td><td>Booked</td><td>4</td><td>Taj Coromandel</td><td>Chennai</td><td>Varun Malhotra</td><td>Chennai</td></tr><tr><td>149</td><td>201</td><td>42</td><td>Delhi</td><td>2025-08-19</td><td>2</td><td>18000.0</td><td>Booked</td><td>4</td><td>The Oberoi</td><td>Delhi</td><td>Ranbir Kapoor</td><td>Delhi</td></tr><tr><td>170</td><td>210</td><td>43</td><td>Bangalore</td><td>2025-03-05</td><td>4</td><td>33000.0</td><td>Booked</td><td>5</td><td>ITC Gardenia</td><td>Bangalore</td><td>Raveena Tandon</td><td>Chennai</td></tr><tr><td>191</td><td>206</td><td>44</td><td>Mumbai</td><td>2025-06-14</td><td>1</td><td>13000.0</td><td>Cancelled</td><td>0</td><td>Trident Nariman Point</td><td>Mumbai</td><td>Taapsee Pannu</td><td>Delhi</td></tr><tr><td>112</td><td>214</td><td>45</td><td>Hyderabad</td><td>2025-09-05</td><td>3</td><td>29000.0</td><td>Booked</td><td>5</td><td>ITC Kohenur</td><td>Hyderabad</td><td>Manoj Joshi</td><td>Kolkata</td></tr><tr><td>133</td><td>217</td><td>46</td><td>Chennai</td><td>2025-04-26</td><td>2</td><td>15000.0</td><td>Booked</td><td>3</td><td>ITC Grand Chola</td><td>Chennai</td><td>Arti Singh</td><td>Hyderabad</td></tr><tr><td>154</td><td>204</td><td>47</td><td>Delhi</td><td>2025-07-22</td><td>5</td><td>37000.0</td><td>Booked</td><td>4</td><td>ITC Maurya</td><td>Delhi</td><td>Amitabh Bachchan</td><td>Kolkata</td></tr><tr><td>175</td><td>208</td><td>48</td><td>Mumbai</td><td>2025-02-10</td><td>3</td><td>31000.0</td><td>Booked</td><td>5</td><td>St. Regis</td><td>Mumbai</td><td>Shakti Kapoor</td><td>Hyderabad</td></tr><tr><td>195</td><td>212</td><td>49</td><td>Bangalore</td><td>2025-05-18</td><td>2</td><td>20000.0</td><td>Cancelled</td><td>0</td><td>Conrad Bengaluru</td><td>Bangalore</td><td>Fatima Sana Shaikh</td><td>Bangalore</td></tr><tr><td>115</td><td>216</td><td>50</td><td>Hyderabad</td><td>2025-08-12</td><td>4</td><td>34000.0</td><td>Booked</td><td>4</td><td>The Westin Mindspace</td><td>Hyderabad</td><td>Sunita Krishnan</td><td>Hyderabad</td></tr><tr><td>135</td><td>218</td><td>51</td><td>Chennai</td><td>2025-01-02</td><td>3</td><td>24000.0</td><td>Booked</td><td>4</td><td>The Leela Palace</td><td>Chennai</td><td>Jayant Sinha</td><td>Bangalore</td></tr><tr><td>155</td><td>202</td><td>52</td><td>Delhi</td><td>2025-04-09</td><td>2</td><td>17500.0</td><td>Booked</td><td>4</td><td>Taj Palace</td><td>Delhi</td><td>Rekha Ganesan</td><td>Delhi</td></tr><tr><td>176</td><td>205</td><td>53</td><td>Mumbai</td><td>2025-07-01</td><td>5</td><td>58000.0</td><td>Booked</td><td>5</td><td>The Taj Mahal Palace</td><td>Mumbai</td><td>Paresh Rawal</td><td>Chennai</td></tr><tr><td>196</td><td>209</td><td>54</td><td>Bangalore</td><td>2025-02-19</td><td>1</td><td>10000.0</td><td>Cancelled</td><td>0</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Zaira Wasim</td><td>Kolkata</td></tr><tr><td>118</td><td>213</td><td>55</td><td>Hyderabad</td><td>2025-06-03</td><td>3</td><td>38000.0</td><td>Booked</td><td>5</td><td>Taj Falaknuma Palace</td><td>Hyderabad</td><td>Sanjay Khanna</td><td>Kolkata</td></tr><tr><td>138</td><td>220</td><td>56</td><td>Chennai</td><td>2025-09-06</td><td>2</td><td>19000.0</td><td>Booked</td><td>4</td><td>Hyatt Regency</td><td>Chennai</td><td>Rani Mukerji</td><td>Mumbai</td></tr><tr><td>158</td><td>203</td><td>57</td><td>Delhi</td><td>2025-03-19</td><td>4</td><td>29000.0</td><td>Booked</td><td>4</td><td>Leela Palace</td><td>Delhi</td><td>Dharmendra Deol</td><td>Chennai</td></tr><tr><td>178</td><td>207</td><td>58</td><td>Mumbai</td><td>2025-08-04</td><td>3</td><td>33000.0</td><td>Booked</td><td>5</td><td>Four Seasons</td><td>Mumbai</td><td>Amrish Puri</td><td>Kolkata</td></tr><tr><td>198</td><td>211</td><td>59</td><td>Bangalore</td><td>2025-05-28</td><td>2</td><td>21000.0</td><td>Cancelled</td><td>0</td><td>Shangri-La</td><td>Bangalore</td><td>Janhvi Kapoor</td><td>Mumbai</td></tr><tr><td>121</td><td>215</td><td>60</td><td>Hyderabad</td><td>2025-01-16</td><td>4</td><td>29500.0</td><td>Booked</td><td>5</td><td>Park Hyatt</td><td>Hyderabad</td><td>Rina Kapoor</td><td>Hyderabad</td></tr><tr><td>141</td><td>209</td><td>61</td><td>Bangalore</td><td>2025-03-23</td><td>3</td><td>28800.0</td><td>Booked</td><td>4</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Hrithik Roshan</td><td>Bangalore</td></tr><tr><td>181</td><td>210</td><td>62</td><td>Bangalore</td><td>2025-08-11</td><td>6</td><td>45000.0</td><td>Cancelled</td><td>0</td><td>ITC Gardenia</td><td>Bangalore</td><td>Irrfan Khan</td><td>Hyderabad</td></tr><tr><td>101</td><td>213</td><td>63</td><td>Hyderabad</td><td>2025-07-05</td><td>5</td><td>55000.0</td><td>Booked</td><td>5</td><td>Taj Falaknuma Palace</td><td>Hyderabad</td><td>Anjali Patil</td><td>Chennai</td></tr><tr><td>161</td><td>211</td><td>64</td><td>Bangalore</td><td>2025-02-19</td><td>2</td><td>18500.0</td><td>Booked</td><td>3</td><td>Shangri-La</td><td>Bangalore</td><td>Madhuri Dixit</td><td>Delhi</td></tr><tr><td>191</td><td>219</td><td>65</td><td>Chennai</td><td>2025-07-01</td><td>4</td><td>34000.0</td><td>Booked</td><td>4</td><td>Taj Coromandel</td><td>Chennai</td><td>Taapsee Pannu</td><td>Delhi</td></tr><tr><td>111</td><td>209</td><td>66</td><td>Bangalore</td><td>2025-09-02</td><td>3</td><td>32000.0</td><td>Booked</td><td>5</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Kavita Nair</td><td>Bangalore</td></tr><tr><td>131</td><td>206</td><td>67</td><td>Mumbai</td><td>2025-04-13</td><td>8</td><td>75000.0</td><td>Cancelled</td><td>0</td><td>Trident Nariman Point</td><td>Mumbai</td><td>Isha Bhatt</td><td>Delhi</td></tr><tr><td>151</td><td>204</td><td>68</td><td>Delhi</td><td>2025-05-06</td><td>4</td><td>28000.0</td><td>Booked</td><td>4</td><td>ITC Maurya</td><td>Delhi</td><td>Salman Khan</td><td>Hyderabad</td></tr><tr><td>171</td><td>218</td><td>69</td><td>Chennai</td><td>2025-08-26</td><td>5</td><td>39000.0</td><td>Booked</td><td>4</td><td>The Leela Palace</td><td>Chennai</td><td>Urmila Matondkar</td><td>Bangalore</td></tr><tr><td>102</td><td>217</td><td>70</td><td>Chennai</td><td>2025-01-21</td><td>3</td><td>26000.0</td><td>Booked</td><td>5</td><td>ITC Grand Chola</td><td>Chennai</td><td>Mahesh Reddy</td><td>Delhi</td></tr><tr><td>123</td><td>205</td><td>71</td><td>Mumbai</td><td>2025-03-15</td><td>4</td><td>42000.0</td><td>Booked</td><td>5</td><td>The Taj Mahal Palace</td><td>Mumbai</td><td>Divya Prasad</td><td>Bangalore</td></tr><tr><td>144</td><td>202</td><td>72</td><td>Delhi</td><td>2025-07-20</td><td>2</td><td>19500.0</td><td>Booked</td><td>3</td><td>Taj Palace</td><td>Delhi</td><td>Deepika Padukone</td><td>Mumbai</td></tr><tr><td>165</td><td>212</td><td>73</td><td>Bangalore</td><td>2025-03-01</td><td>6</td><td>51000.0</td><td>Cancelled</td><td>0</td><td>Conrad Bengaluru</td><td>Bangalore</td><td>Kajol Devgan</td><td>Bangalore</td></tr><tr><td>184</td><td>216</td><td>74</td><td>Hyderabad</td><td>2025-06-10</td><td>3</td><td>27500.0</td><td>Booked</td><td>4</td><td>The Westin Mindspace</td><td>Hyderabad</td><td>Manoj Bajpayee</td><td>Kolkata</td></tr><tr><td>105</td><td>209</td><td>75</td><td>Bangalore</td><td>2025-09-04</td><td>4</td><td>38000.0</td><td>Booked</td><td>5</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Pooja Verma</td><td>Kolkata</td></tr><tr><td>125</td><td>214</td><td>76</td><td>Hyderabad</td><td>2025-04-22</td><td>5</td><td>43000.0</td><td>Booked</td><td>4</td><td>ITC Kohenur</td><td>Hyderabad</td><td>Lata Mistry</td><td>Delhi</td></tr><tr><td>146</td><td>208</td><td>77</td><td>Mumbai</td><td>2025-08-17</td><td>3</td><td>34000.0</td><td>Booked</td><td>5</td><td>St. Regis</td><td>Mumbai</td><td>Priyanka Chopra</td><td>Chennai</td></tr><tr><td>167</td><td>220</td><td>78</td><td>Chennai</td><td>2025-01-12</td><td>7</td><td>49000.0</td><td>Cancelled</td><td>0</td><td>Hyatt Regency</td><td>Chennai</td><td>Akshay Kumar</td><td>Delhi</td></tr><tr><td>189</td><td>203</td><td>79</td><td>Delhi</td><td>2025-05-31</td><td>4</td><td>31000.0</td><td>Booked</td><td>4</td><td>Leela Palace</td><td>Delhi</td><td>Kangana Ranaut</td><td>Bangalore</td></tr><tr><td>110</td><td>210</td><td>80</td><td>Bangalore</td><td>2025-07-28</td><td>3</td><td>29000.0</td><td>Booked</td><td>4</td><td>ITC Gardenia</td><td>Bangalore</td><td>Arjun Rao</td><td>Chennai</td></tr><tr><td>130</td><td>206</td><td>81</td><td>Mumbai</td><td>2025-02-05</td><td>5</td><td>46000.0</td><td>Booked</td><td>5</td><td>Trident Nariman Point</td><td>Mumbai</td><td>Gopal Varma</td><td>Kolkata</td></tr><tr><td>150</td><td>201</td><td>82</td><td>Delhi</td><td>2025-06-19</td><td>2</td><td>21000.0</td><td>Booked</td><td>4</td><td>The Oberoi</td><td>Delhi</td><td>Katrina Kaif</td><td>Mumbai</td></tr><tr><td>172</td><td>219</td><td>83</td><td>Chennai</td><td>2025-09-03</td><td>4</td><td>35500.0</td><td>Cancelled</td><td>0</td><td>Taj Coromandel</td><td>Chennai</td><td>Manisha Koirala</td><td>Kolkata</td></tr><tr><td>192</td><td>215</td><td>84</td><td>Hyderabad</td><td>2025-03-10</td><td>6</td><td>48000.0</td><td>Booked</td><td>5</td><td>Park Hyatt</td><td>Hyderabad</td><td>Bhumi Pednekar</td><td>Mumbai</td></tr><tr><td>114</td><td>218</td><td>85</td><td>Chennai</td><td>2025-08-08</td><td>3</td><td>27000.0</td><td>Booked</td><td>3</td><td>The Leela Palace</td><td>Chennai</td><td>Rajesh Menon</td><td>Mumbai</td></tr><tr><td>134</td><td>204</td><td>86</td><td>Delhi</td><td>2025-01-26</td><td>5</td><td>36000.0</td><td>Booked</td><td>4</td><td>ITC Maurya</td><td>Delhi</td><td>Kiran Bedi</td><td>Chennai</td></tr><tr><td>156</td><td>207</td><td>87</td><td>Mumbai</td><td>2025-04-04</td><td>4</td><td>38000.0</td><td>Booked</td><td>5</td><td>Four Seasons</td><td>Mumbai</td><td>Jaya Bhaduri</td><td>Mumbai</td></tr><tr><td>177</td><td>211</td><td>88</td><td>Bangalore</td><td>2025-07-14</td><td>3</td><td>28500.0</td><td>Cancelled</td><td>0</td><td>Shangri-La</td><td>Bangalore</td><td>Anupam Kher</td><td>Bangalore</td></tr><tr><td>197</td><td>213</td><td>89</td><td>Hyderabad</td><td>2025-02-15</td><td>7</td><td>72000.0</td><td>Booked</td><td>5</td><td>Taj Falaknuma Palace</td><td>Hyderabad</td><td>Ananya Panday</td><td>Delhi</td></tr><tr><td>119</td><td>205</td><td>90</td><td>Mumbai</td><td>2025-05-24</td><td>3</td><td>33000.0</td><td>Booked</td><td>4</td><td>The Taj Mahal Palace</td><td>Mumbai</td><td>Geeta Biswas</td><td>Delhi</td></tr><tr><td>139</td><td>209</td><td>91</td><td>Bangalore</td><td>2025-01-09</td><td>5</td><td>47000.0</td><td>Booked</td><td>5</td><td>The Ritz-Carlton</td><td>Bangalore</td><td>Abhishek Bachchan</td><td>Hyderabad</td></tr><tr><td>157</td><td>217</td><td>92</td><td>Chennai</td><td>2025-09-01</td><td>4</td><td>33000.0</td><td>Booked</td><td>4</td><td>ITC Grand Chola</td><td>Chennai</td><td>Hema Malini</td><td>Hyderabad</td></tr><tr><td>180</td><td>214</td><td>93</td><td>Hyderabad</td><td>2025-03-29</td><td>2</td><td>22000.0</td><td>Cancelled</td><td>0</td><td>ITC Kohenur</td><td>Hyderabad</td><td>Naseeruddin Shah</td><td>Mumbai</td></tr><tr><td>108</td><td>208</td><td>94</td><td>Mumbai</td><td>2025-06-26</td><td>6</td><td>65000.0</td><td>Booked</td><td>5</td><td>St. Regis</td><td>Mumbai</td><td>Vikram Singh</td><td>Mumbai</td></tr><tr><td>129</td><td>202</td><td>95</td><td>Delhi</td><td>2025-09-05</td><td>3</td><td>29000.0</td><td>Booked</td><td>4</td><td>Taj Palace</td><td>Delhi</td><td>Meena Kumari</td><td>Bangalore</td></tr><tr><td>147</td><td>212</td><td>96</td><td>Bangalore</td><td>2025-04-18</td><td>4</td><td>36500.0</td><td>Booked</td><td>4</td><td>Conrad Bengaluru</td><td>Bangalore</td><td>Shahid Kapoor</td><td>Bangalore</td></tr><tr><td>168</td><td>220</td><td>97</td><td>Chennai</td><td>2025-07-09</td><td>5</td><td>38000.0</td><td>Booked</td><td>3</td><td>Hyatt Regency</td><td>Chennai</td><td>Twinkle Khanna</td><td>Mumbai</td></tr><tr><td>188</td><td>216</td><td>98</td><td>Hyderabad</td><td>2025-02-23</td><td>3</td><td>28000.0</td><td>Cancelled</td><td>0</td><td>The Westin Mindspace</td><td>Hyderabad</td><td>Sushant Singh Rajput</td><td>Chennai</td></tr><tr><td>193</td><td>203</td><td>99</td><td>Delhi</td><td>2025-05-12</td><td>6</td><td>44000.0</td><td>Booked</td><td>5</td><td>Leela Palace</td><td>Delhi</td><td>Radhika Apte</td><td>Hyderabad</td></tr><tr><td>126</td><td>219</td><td>100</td><td>Chennai</td><td>2025-01-31</td><td>4</td><td>32500.0</td><td>Booked</td><td>4</td><td>Taj Coromandel</td><td>Chennai</td><td>Tarun Bajaj</td><td>Mumbai</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "121",
         "215",
         "1",
         "Hyderabad",
         "2025-01-15",
         3,
         21500.0,
         "Booked",
         "5",
         "Park Hyatt",
         "Hyderabad",
         "Rina Kapoor",
         "Hyderabad"
        ],
        [
         "142",
         "207",
         "2",
         "Mumbai",
         "2025-03-22",
         2,
         19800.0,
         "Booked",
         "4",
         "Four Seasons",
         "Mumbai",
         "Kareena Kapoor",
         "Kolkata"
        ],
        [
         "183",
         "210",
         "3",
         "Bangalore",
         "2025-08-10",
         5,
         35000.0,
         "Cancelled",
         "0",
         "ITC Gardenia",
         "Bangalore",
         "Pankaj Tripathi",
         "Bangalore"
        ],
        [
         "104",
         "213",
         "4",
         "Hyderabad",
         "2025-07-04",
         4,
         45000.0,
         "Booked",
         "5",
         "Taj Falaknuma Palace",
         "Hyderabad",
         "Rohan Mehra",
         "Hyderabad"
        ],
        [
         "165",
         "211",
         "5",
         "Bangalore",
         "2025-02-18",
         1,
         8500.0,
         "Booked",
         "3",
         "Shangri-La",
         "Bangalore",
         "Kajol Devgan",
         "Bangalore"
        ],
        [
         "196",
         "219",
         "6",
         "Chennai",
         "2025-06-30",
         3,
         24000.0,
         "Booked",
         "4",
         "Taj Coromandel",
         "Chennai",
         "Zaira Wasim",
         "Kolkata"
        ],
        [
         "117",
         "209",
         "7",
         "Bangalore",
         "2025-09-01",
         2,
         22000.0,
         "Booked",
         "5",
         "The Ritz-Carlton",
         "Bangalore",
         "Priya Sharma",
         "Bangalore"
        ],
        [
         "138",
         "206",
         "8",
         "Mumbai",
         "2025-04-12",
         7,
         65000.0,
         "Cancelled",
         "0",
         "Trident Nariman Point",
         "Mumbai",
         "Rani Mukerji",
         "Mumbai"
        ],
        [
         "159",
         "204",
         "9",
         "Delhi",
         "2025-05-05",
         3,
         18000.0,
         "Booked",
         "4",
         "ITC Maurya",
         "Delhi",
         "Sridevi Kapoor",
         "Bangalore"
        ],
        [
         "170",
         "218",
         "10",
         "Chennai",
         "2025-08-25",
         4,
         29000.0,
         "Booked",
         "4",
         "The Leela Palace",
         "Chennai",
         "Raveena Tandon",
         "Chennai"
        ],
        [
         "101",
         "217",
         "11",
         "Chennai",
         "2025-01-20",
         2,
         16000.0,
         "Booked",
         "5",
         "ITC Grand Chola",
         "Chennai",
         "Anjali Patil",
         "Chennai"
        ],
        [
         "122",
         "205",
         "12",
         "Mumbai",
         "2025-03-14",
         3,
         32000.0,
         "Booked",
         "5",
         "The Taj Mahal Palace",
         "Mumbai",
         "Sameer Ali",
         "Chennai"
        ],
        [
         "143",
         "202",
         "13",
         "Delhi",
         "2025-07-19",
         1,
         9500.0,
         "Booked",
         "3",
         "Taj Palace",
         "Delhi",
         "Saif Ali Khan",
         "Delhi"
        ],
        [
         "164",
         "212",
         "14",
         "Bangalore",
         "2025-02-28",
         5,
         41000.0,
         "Cancelled",
         "0",
         "Conrad Bengaluru",
         "Bangalore",
         "Govinda Ahuja",
         "Chennai"
        ],
        [
         "185",
         "216",
         "15",
         "Hyderabad",
         "2025-06-09",
         2,
         17500.0,
         "Booked",
         "4",
         "The Westin Mindspace",
         "Hyderabad",
         "Rajkummar Rao",
         "Delhi"
        ],
        [
         "106",
         "209",
         "16",
         "Bangalore",
         "2025-09-03",
         3,
         28000.0,
         "Booked",
         "5",
         "The Ritz-Carlton",
         "Bangalore",
         "Amit Kumar",
         "Bangalore"
        ],
        [
         "127",
         "214",
         "17",
         "Hyderabad",
         "2025-04-21",
         4,
         33000.0,
         "Booked",
         "4",
         "ITC Kohenur",
         "Hyderabad",
         "Swati Pandit",
         "Hyderabad"
        ],
        [
         "148",
         "208",
         "18",
         "Mumbai",
         "2025-08-16",
         2,
         24000.0,
         "Booked",
         "5",
         "St. Regis",
         "Mumbai",
         "Alia Bhatt",
         "Kolkata"
        ],
        [
         "169",
         "220",
         "19",
         "Chennai",
         "2025-01-11",
         6,
         39000.0,
         "Cancelled",
         "0",
         "Hyatt Regency",
         "Chennai",
         "Suniel Shetty",
         "Hyderabad"
        ],
        [
         "190",
         "203",
         "20",
         "Delhi",
         "2025-05-30",
         3,
         21000.0,
         "Booked",
         "4",
         "Leela Palace",
         "Delhi",
         "Vidya Balan",
         "Kolkata"
        ],
        [
         "111",
         "210",
         "21",
         "Bangalore",
         "2025-07-27",
         2,
         19000.0,
         "Booked",
         "4",
         "ITC Gardenia",
         "Bangalore",
         "Kavita Nair",
         "Bangalore"
        ],
        [
         "132",
         "206",
         "22",
         "Mumbai",
         "2025-02-04",
         4,
         36000.0,
         "Booked",
         "5",
         "Trident Nariman Point",
         "Mumbai",
         "Nitin Saxena",
         "Mumbai"
        ],
        [
         "153",
         "201",
         "23",
         "Delhi",
         "2025-06-18",
         1,
         11000.0,
         "Booked",
         "4",
         "The Oberoi",
         "Delhi",
         "Aamir Khan",
         "Bangalore"
        ],
        [
         "174",
         "219",
         "24",
         "Chennai",
         "2025-09-02",
         3,
         25500.0,
         "Cancelled",
         "0",
         "Taj Coromandel",
         "Chennai",
         "Karisma Kapoor",
         "Mumbai"
        ],
        [
         "195",
         "215",
         "25",
         "Hyderabad",
         "2025-03-09",
         5,
         38000.0,
         "Booked",
         "5",
         "Park Hyatt",
         "Hyderabad",
         "Fatima Sana Shaikh",
         "Bangalore"
        ],
        [
         "116",
         "218",
         "26",
         "Chennai",
         "2025-08-07",
         2,
         17000.0,
         "Booked",
         "3",
         "The Leela Palace",
         "Chennai",
         "Aditya Narayan",
         "Chennai"
        ],
        [
         "137",
         "204",
         "27",
         "Delhi",
         "2025-01-25",
         4,
         26000.0,
         "Booked",
         "4",
         "ITC Maurya",
         "Delhi",
         "Vivek Oberoi",
         "Delhi"
        ],
        [
         "158",
         "207",
         "28",
         "Mumbai",
         "2025-04-03",
         3,
         28000.0,
         "Booked",
         "5",
         "Four Seasons",
         "Mumbai",
         "Dharmendra Deol",
         "Chennai"
        ],
        [
         "179",
         "211",
         "29",
         "Bangalore",
         "2025-07-13",
         2,
         18500.0,
         "Cancelled",
         "0",
         "Shangri-La",
         "Bangalore",
         "Om Puri",
         "Delhi"
        ],
        [
         "199",
         "213",
         "30",
         "Hyderabad",
         "2025-02-14",
         6,
         62000.0,
         "Booked",
         "5",
         "Taj Falaknuma Palace",
         "Hyderabad",
         "Sara Ali Khan",
         "Hyderabad"
        ],
        [
         "120",
         "205",
         "31",
         "Mumbai",
         "2025-05-23",
         2,
         23000.0,
         "Booked",
         "4",
         "The Taj Mahal Palace",
         "Mumbai",
         "Anil Mehta",
         "Mumbai"
        ],
        [
         "141",
         "209",
         "32",
         "Bangalore",
         "2025-01-08",
         4,
         37000.0,
         "Booked",
         "5",
         "The Ritz-Carlton",
         "Bangalore",
         "Hrithik Roshan",
         "Bangalore"
        ],
        [
         "162",
         "217",
         "33",
         "Chennai",
         "2025-08-31",
         3,
         23000.0,
         "Booked",
         "4",
         "ITC Grand Chola",
         "Chennai",
         "Sanjay Dutt",
         "Mumbai"
        ],
        [
         "182",
         "214",
         "34",
         "Hyderabad",
         "2025-03-28",
         1,
         12000.0,
         "Cancelled",
         "0",
         "ITC Kohenur",
         "Hyderabad",
         "Nawazuddin Siddiqui",
         "Chennai"
        ],
        [
         "103",
         "208",
         "35",
         "Mumbai",
         "2025-06-25",
         5,
         55000.0,
         "Booked",
         "5",
         "St. Regis",
         "Mumbai",
         "Sarika Desai",
         "Mumbai"
        ],
        [
         "124",
         "202",
         "36",
         "Delhi",
         "2025-09-04",
         2,
         19000.0,
         "Booked",
         "4",
         "Taj Palace",
         "Delhi",
         "Harish Chandra",
         "Kolkata"
        ],
        [
         "145",
         "212",
         "37",
         "Bangalore",
         "2025-04-17",
         3,
         26500.0,
         "Booked",
         "4",
         "Conrad Bengaluru",
         "Bangalore",
         "Ranveer Singh",
         "Hyderabad"
        ],
        [
         "166",
         "220",
         "38",
         "Chennai",
         "2025-07-08",
         4,
         28000.0,
         "Booked",
         "3",
         "Hyatt Regency",
         "Chennai",
         "Ajay Devgan",
         "Kolkata"
        ],
        [
         "186",
         "216",
         "39",
         "Hyderabad",
         "2025-02-22",
         2,
         18000.0,
         "Cancelled",
         "0",
         "The Westin Mindspace",
         "Hyderabad",
         "Ayushmann Khurrana",
         "Mumbai"
        ],
        [
         "107",
         "203",
         "40",
         "Delhi",
         "2025-05-11",
         5,
         34000.0,
         "Booked",
         "5",
         "Leela Palace",
         "Delhi",
         "Sneha Shah",
         "Delhi"
        ],
        [
         "128",
         "219",
         "41",
         "Chennai",
         "2025-01-30",
         3,
         22500.0,
         "Booked",
         "4",
         "Taj Coromandel",
         "Chennai",
         "Varun Malhotra",
         "Chennai"
        ],
        [
         "149",
         "201",
         "42",
         "Delhi",
         "2025-08-19",
         2,
         18000.0,
         "Booked",
         "4",
         "The Oberoi",
         "Delhi",
         "Ranbir Kapoor",
         "Delhi"
        ],
        [
         "170",
         "210",
         "43",
         "Bangalore",
         "2025-03-05",
         4,
         33000.0,
         "Booked",
         "5",
         "ITC Gardenia",
         "Bangalore",
         "Raveena Tandon",
         "Chennai"
        ],
        [
         "191",
         "206",
         "44",
         "Mumbai",
         "2025-06-14",
         1,
         13000.0,
         "Cancelled",
         "0",
         "Trident Nariman Point",
         "Mumbai",
         "Taapsee Pannu",
         "Delhi"
        ],
        [
         "112",
         "214",
         "45",
         "Hyderabad",
         "2025-09-05",
         3,
         29000.0,
         "Booked",
         "5",
         "ITC Kohenur",
         "Hyderabad",
         "Manoj Joshi",
         "Kolkata"
        ],
        [
         "133",
         "217",
         "46",
         "Chennai",
         "2025-04-26",
         2,
         15000.0,
         "Booked",
         "3",
         "ITC Grand Chola",
         "Chennai",
         "Arti Singh",
         "Hyderabad"
        ],
        [
         "154",
         "204",
         "47",
         "Delhi",
         "2025-07-22",
         5,
         37000.0,
         "Booked",
         "4",
         "ITC Maurya",
         "Delhi",
         "Amitabh Bachchan",
         "Kolkata"
        ],
        [
         "175",
         "208",
         "48",
         "Mumbai",
         "2025-02-10",
         3,
         31000.0,
         "Booked",
         "5",
         "St. Regis",
         "Mumbai",
         "Shakti Kapoor",
         "Hyderabad"
        ],
        [
         "195",
         "212",
         "49",
         "Bangalore",
         "2025-05-18",
         2,
         20000.0,
         "Cancelled",
         "0",
         "Conrad Bengaluru",
         "Bangalore",
         "Fatima Sana Shaikh",
         "Bangalore"
        ],
        [
         "115",
         "216",
         "50",
         "Hyderabad",
         "2025-08-12",
         4,
         34000.0,
         "Booked",
         "4",
         "The Westin Mindspace",
         "Hyderabad",
         "Sunita Krishnan",
         "Hyderabad"
        ],
        [
         "135",
         "218",
         "51",
         "Chennai",
         "2025-01-02",
         3,
         24000.0,
         "Booked",
         "4",
         "The Leela Palace",
         "Chennai",
         "Jayant Sinha",
         "Bangalore"
        ],
        [
         "155",
         "202",
         "52",
         "Delhi",
         "2025-04-09",
         2,
         17500.0,
         "Booked",
         "4",
         "Taj Palace",
         "Delhi",
         "Rekha Ganesan",
         "Delhi"
        ],
        [
         "176",
         "205",
         "53",
         "Mumbai",
         "2025-07-01",
         5,
         58000.0,
         "Booked",
         "5",
         "The Taj Mahal Palace",
         "Mumbai",
         "Paresh Rawal",
         "Chennai"
        ],
        [
         "196",
         "209",
         "54",
         "Bangalore",
         "2025-02-19",
         1,
         10000.0,
         "Cancelled",
         "0",
         "The Ritz-Carlton",
         "Bangalore",
         "Zaira Wasim",
         "Kolkata"
        ],
        [
         "118",
         "213",
         "55",
         "Hyderabad",
         "2025-06-03",
         3,
         38000.0,
         "Booked",
         "5",
         "Taj Falaknuma Palace",
         "Hyderabad",
         "Sanjay Khanna",
         "Kolkata"
        ],
        [
         "138",
         "220",
         "56",
         "Chennai",
         "2025-09-06",
         2,
         19000.0,
         "Booked",
         "4",
         "Hyatt Regency",
         "Chennai",
         "Rani Mukerji",
         "Mumbai"
        ],
        [
         "158",
         "203",
         "57",
         "Delhi",
         "2025-03-19",
         4,
         29000.0,
         "Booked",
         "4",
         "Leela Palace",
         "Delhi",
         "Dharmendra Deol",
         "Chennai"
        ],
        [
         "178",
         "207",
         "58",
         "Mumbai",
         "2025-08-04",
         3,
         33000.0,
         "Booked",
         "5",
         "Four Seasons",
         "Mumbai",
         "Amrish Puri",
         "Kolkata"
        ],
        [
         "198",
         "211",
         "59",
         "Bangalore",
         "2025-05-28",
         2,
         21000.0,
         "Cancelled",
         "0",
         "Shangri-La",
         "Bangalore",
         "Janhvi Kapoor",
         "Mumbai"
        ],
        [
         "121",
         "215",
         "60",
         "Hyderabad",
         "2025-01-16",
         4,
         29500.0,
         "Booked",
         "5",
         "Park Hyatt",
         "Hyderabad",
         "Rina Kapoor",
         "Hyderabad"
        ],
        [
         "141",
         "209",
         "61",
         "Bangalore",
         "2025-03-23",
         3,
         28800.0,
         "Booked",
         "4",
         "The Ritz-Carlton",
         "Bangalore",
         "Hrithik Roshan",
         "Bangalore"
        ],
        [
         "181",
         "210",
         "62",
         "Bangalore",
         "2025-08-11",
         6,
         45000.0,
         "Cancelled",
         "0",
         "ITC Gardenia",
         "Bangalore",
         "Irrfan Khan",
         "Hyderabad"
        ],
        [
         "101",
         "213",
         "63",
         "Hyderabad",
         "2025-07-05",
         5,
         55000.0,
         "Booked",
         "5",
         "Taj Falaknuma Palace",
         "Hyderabad",
         "Anjali Patil",
         "Chennai"
        ],
        [
         "161",
         "211",
         "64",
         "Bangalore",
         "2025-02-19",
         2,
         18500.0,
         "Booked",
         "3",
         "Shangri-La",
         "Bangalore",
         "Madhuri Dixit",
         "Delhi"
        ],
        [
         "191",
         "219",
         "65",
         "Chennai",
         "2025-07-01",
         4,
         34000.0,
         "Booked",
         "4",
         "Taj Coromandel",
         "Chennai",
         "Taapsee Pannu",
         "Delhi"
        ],
        [
         "111",
         "209",
         "66",
         "Bangalore",
         "2025-09-02",
         3,
         32000.0,
         "Booked",
         "5",
         "The Ritz-Carlton",
         "Bangalore",
         "Kavita Nair",
         "Bangalore"
        ],
        [
         "131",
         "206",
         "67",
         "Mumbai",
         "2025-04-13",
         8,
         75000.0,
         "Cancelled",
         "0",
         "Trident Nariman Point",
         "Mumbai",
         "Isha Bhatt",
         "Delhi"
        ],
        [
         "151",
         "204",
         "68",
         "Delhi",
         "2025-05-06",
         4,
         28000.0,
         "Booked",
         "4",
         "ITC Maurya",
         "Delhi",
         "Salman Khan",
         "Hyderabad"
        ],
        [
         "171",
         "218",
         "69",
         "Chennai",
         "2025-08-26",
         5,
         39000.0,
         "Booked",
         "4",
         "The Leela Palace",
         "Chennai",
         "Urmila Matondkar",
         "Bangalore"
        ],
        [
         "102",
         "217",
         "70",
         "Chennai",
         "2025-01-21",
         3,
         26000.0,
         "Booked",
         "5",
         "ITC Grand Chola",
         "Chennai",
         "Mahesh Reddy",
         "Delhi"
        ],
        [
         "123",
         "205",
         "71",
         "Mumbai",
         "2025-03-15",
         4,
         42000.0,
         "Booked",
         "5",
         "The Taj Mahal Palace",
         "Mumbai",
         "Divya Prasad",
         "Bangalore"
        ],
        [
         "144",
         "202",
         "72",
         "Delhi",
         "2025-07-20",
         2,
         19500.0,
         "Booked",
         "3",
         "Taj Palace",
         "Delhi",
         "Deepika Padukone",
         "Mumbai"
        ],
        [
         "165",
         "212",
         "73",
         "Bangalore",
         "2025-03-01",
         6,
         51000.0,
         "Cancelled",
         "0",
         "Conrad Bengaluru",
         "Bangalore",
         "Kajol Devgan",
         "Bangalore"
        ],
        [
         "184",
         "216",
         "74",
         "Hyderabad",
         "2025-06-10",
         3,
         27500.0,
         "Booked",
         "4",
         "The Westin Mindspace",
         "Hyderabad",
         "Manoj Bajpayee",
         "Kolkata"
        ],
        [
         "105",
         "209",
         "75",
         "Bangalore",
         "2025-09-04",
         4,
         38000.0,
         "Booked",
         "5",
         "The Ritz-Carlton",
         "Bangalore",
         "Pooja Verma",
         "Kolkata"
        ],
        [
         "125",
         "214",
         "76",
         "Hyderabad",
         "2025-04-22",
         5,
         43000.0,
         "Booked",
         "4",
         "ITC Kohenur",
         "Hyderabad",
         "Lata Mistry",
         "Delhi"
        ],
        [
         "146",
         "208",
         "77",
         "Mumbai",
         "2025-08-17",
         3,
         34000.0,
         "Booked",
         "5",
         "St. Regis",
         "Mumbai",
         "Priyanka Chopra",
         "Chennai"
        ],
        [
         "167",
         "220",
         "78",
         "Chennai",
         "2025-01-12",
         7,
         49000.0,
         "Cancelled",
         "0",
         "Hyatt Regency",
         "Chennai",
         "Akshay Kumar",
         "Delhi"
        ],
        [
         "189",
         "203",
         "79",
         "Delhi",
         "2025-05-31",
         4,
         31000.0,
         "Booked",
         "4",
         "Leela Palace",
         "Delhi",
         "Kangana Ranaut",
         "Bangalore"
        ],
        [
         "110",
         "210",
         "80",
         "Bangalore",
         "2025-07-28",
         3,
         29000.0,
         "Booked",
         "4",
         "ITC Gardenia",
         "Bangalore",
         "Arjun Rao",
         "Chennai"
        ],
        [
         "130",
         "206",
         "81",
         "Mumbai",
         "2025-02-05",
         5,
         46000.0,
         "Booked",
         "5",
         "Trident Nariman Point",
         "Mumbai",
         "Gopal Varma",
         "Kolkata"
        ],
        [
         "150",
         "201",
         "82",
         "Delhi",
         "2025-06-19",
         2,
         21000.0,
         "Booked",
         "4",
         "The Oberoi",
         "Delhi",
         "Katrina Kaif",
         "Mumbai"
        ],
        [
         "172",
         "219",
         "83",
         "Chennai",
         "2025-09-03",
         4,
         35500.0,
         "Cancelled",
         "0",
         "Taj Coromandel",
         "Chennai",
         "Manisha Koirala",
         "Kolkata"
        ],
        [
         "192",
         "215",
         "84",
         "Hyderabad",
         "2025-03-10",
         6,
         48000.0,
         "Booked",
         "5",
         "Park Hyatt",
         "Hyderabad",
         "Bhumi Pednekar",
         "Mumbai"
        ],
        [
         "114",
         "218",
         "85",
         "Chennai",
         "2025-08-08",
         3,
         27000.0,
         "Booked",
         "3",
         "The Leela Palace",
         "Chennai",
         "Rajesh Menon",
         "Mumbai"
        ],
        [
         "134",
         "204",
         "86",
         "Delhi",
         "2025-01-26",
         5,
         36000.0,
         "Booked",
         "4",
         "ITC Maurya",
         "Delhi",
         "Kiran Bedi",
         "Chennai"
        ],
        [
         "156",
         "207",
         "87",
         "Mumbai",
         "2025-04-04",
         4,
         38000.0,
         "Booked",
         "5",
         "Four Seasons",
         "Mumbai",
         "Jaya Bhaduri",
         "Mumbai"
        ],
        [
         "177",
         "211",
         "88",
         "Bangalore",
         "2025-07-14",
         3,
         28500.0,
         "Cancelled",
         "0",
         "Shangri-La",
         "Bangalore",
         "Anupam Kher",
         "Bangalore"
        ],
        [
         "197",
         "213",
         "89",
         "Hyderabad",
         "2025-02-15",
         7,
         72000.0,
         "Booked",
         "5",
         "Taj Falaknuma Palace",
         "Hyderabad",
         "Ananya Panday",
         "Delhi"
        ],
        [
         "119",
         "205",
         "90",
         "Mumbai",
         "2025-05-24",
         3,
         33000.0,
         "Booked",
         "4",
         "The Taj Mahal Palace",
         "Mumbai",
         "Geeta Biswas",
         "Delhi"
        ],
        [
         "139",
         "209",
         "91",
         "Bangalore",
         "2025-01-09",
         5,
         47000.0,
         "Booked",
         "5",
         "The Ritz-Carlton",
         "Bangalore",
         "Abhishek Bachchan",
         "Hyderabad"
        ],
        [
         "157",
         "217",
         "92",
         "Chennai",
         "2025-09-01",
         4,
         33000.0,
         "Booked",
         "4",
         "ITC Grand Chola",
         "Chennai",
         "Hema Malini",
         "Hyderabad"
        ],
        [
         "180",
         "214",
         "93",
         "Hyderabad",
         "2025-03-29",
         2,
         22000.0,
         "Cancelled",
         "0",
         "ITC Kohenur",
         "Hyderabad",
         "Naseeruddin Shah",
         "Mumbai"
        ],
        [
         "108",
         "208",
         "94",
         "Mumbai",
         "2025-06-26",
         6,
         65000.0,
         "Booked",
         "5",
         "St. Regis",
         "Mumbai",
         "Vikram Singh",
         "Mumbai"
        ],
        [
         "129",
         "202",
         "95",
         "Delhi",
         "2025-09-05",
         3,
         29000.0,
         "Booked",
         "4",
         "Taj Palace",
         "Delhi",
         "Meena Kumari",
         "Bangalore"
        ],
        [
         "147",
         "212",
         "96",
         "Bangalore",
         "2025-04-18",
         4,
         36500.0,
         "Booked",
         "4",
         "Conrad Bengaluru",
         "Bangalore",
         "Shahid Kapoor",
         "Bangalore"
        ],
        [
         "168",
         "220",
         "97",
         "Chennai",
         "2025-07-09",
         5,
         38000.0,
         "Booked",
         "3",
         "Hyatt Regency",
         "Chennai",
         "Twinkle Khanna",
         "Mumbai"
        ],
        [
         "188",
         "216",
         "98",
         "Hyderabad",
         "2025-02-23",
         3,
         28000.0,
         "Cancelled",
         "0",
         "The Westin Mindspace",
         "Hyderabad",
         "Sushant Singh Rajput",
         "Chennai"
        ],
        [
         "193",
         "203",
         "99",
         "Delhi",
         "2025-05-12",
         6,
         44000.0,
         "Booked",
         "5",
         "Leela Palace",
         "Delhi",
         "Radhika Apte",
         "Hyderabad"
        ],
        [
         "126",
         "219",
         "100",
         "Chennai",
         "2025-01-31",
         4,
         32500.0,
         "Booked",
         "4",
         "Taj Coromandel",
         "Chennai",
         "Tarun Bajaj",
         "Mumbai"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hotel_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "booking_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "booking_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "stay_duration",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "amount_spent",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rating",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hotel_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": {
        "addedWidgets": {},
        "arguments": {},
        "datasetInfos": [],
        "jupyterProps": {
         "ename": "UnsupportedOperationException",
         "evalue": "Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
        },
        "metadata": {},
        "removedWidgets": [],
        "sqlProps": {
         "breakingChangeInfo": null,
         "errorClass": "",
         "pysparkCallSite": "",
         "pysparkFragment": "",
         "pysparkSummary": "",
         "sqlState": "XXKCM",
         "stackTrace": "java.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
         "startIndex": null,
         "stopIndex": null
        },
        "stackFrames": [
         "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
         "\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)",
         "File \u001B[0;32m<command-5866084564046122>, line 16\u001B[0m\n\u001B[1;32m     12\u001B[0m     exec(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;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\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# run user code\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     __backend_agg_user_code_fn()\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m#reset display function\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     display \u001B[38;5;241m=\u001B[39m __backend_agg_display_orig\n",
         "File \u001B[0;32m<command-5866084564046122>, line 12\u001B[0m, in \u001B[0;36m__backend_agg_user_code_fn\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__backend_agg_user_code_fn\u001B[39m():\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m     exec(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;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\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
         "File \u001B[0;32m<string>:64\u001B[0m\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:807\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m    790\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    791\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    805\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m    806\u001B[0m )\n\u001B[0;32m--> 807\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:679\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m--> 679\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    681\u001B[0m )\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1481\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1479\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1480\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1481\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1482\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1483\u001B[0m )\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1970\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1967\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1969\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1970\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1971\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1972\u001B[0m     ):\n\u001B[1;32m   1973\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1974\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1946\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1944\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1945\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1946\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2266\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2264\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2377\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2363\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2364\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2365\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2373\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n\u001B[1;32m   2374\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2375\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2377\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2378\u001B[0m                 info,\n\u001B[1;32m   2379\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2380\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2381\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2382\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2384\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2385\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2386\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2387\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2388\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
         "\u001B[0;31mUnsupportedOperationException\u001B[0m: Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
        ],
        "type": "baseError"
       },
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewadf5442\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewadf5442\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewadf5442\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewadf5442) SELECT `hotel_name`,SUM(`amount_spent`) `column_aa88d480135` FROM q GROUP BY `hotel_name`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewadf5442\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "hotel_name",
             "id": "column_aa88d480133"
            },
            "y": [
             {
              "column": "amount_spent",
              "id": "column_aa88d480135",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_aa88d480135": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)\nFile \u001B[0;32m<command-5866084564046122>, line 16\u001B[0m\n\u001B[1;32m     12\u001B[0m     exec(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;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\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# run user code\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     __backend_agg_user_code_fn()\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m#reset display function\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     display \u001B[38;5;241m=\u001B[39m __backend_agg_display_orig\n\nFile \u001B[0;32m<command-5866084564046122>, line 12\u001B[0m, in \u001B[0;36m__backend_agg_user_code_fn\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__backend_agg_user_code_fn\u001B[39m():\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m     exec(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;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\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\nFile \u001B[0;32m<string>:64\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:807\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m    790\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    791\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    805\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m    806\u001B[0m )\n\u001B[0;32m--> 807\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n\nFile \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:679\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m--> 679\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    681\u001B[0m )\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n\nFile \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1481\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1479\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1480\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1481\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1482\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1483\u001B[0m )\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n\nFile \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1970\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1967\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1969\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1970\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1971\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1972\u001B[0m     ):\n\u001B[1;32m   1973\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1974\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n\nFile \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1946\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1944\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1945\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1946\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n\nFile \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2266\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2264\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n\nFile \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2377\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2363\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2364\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2365\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2373\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n\u001B[1;32m   2374\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2375\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2377\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2378\u001B[0m                 info,\n\u001B[1;32m   2379\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2380\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2381\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2382\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2384\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2385\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2386\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2387\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2388\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\n\u001B[0;31mUnsupportedOperationException\u001B[0m: Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
       "errorDetails": null,
       "errorSummary": "Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
       "errorTraceType": "baseError",
       "finishTime": 1756896042630,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "1b9214c0-f4b3-4acb-b9e4-d4684c845542",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 51.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": {
        "addedWidgets": {},
        "arguments": {},
        "data": [],
        "datasetInfos": [],
        "metadata": {
         "isDbfsCommandResult": false
        },
        "removedWidgets": [],
        "type": "listResults"
       },
       "showCommandTitle": false,
       "startTime": 1756896038506,
       "state": "error",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "hotel_name",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "hotel_name",
           "type": "column"
          },
          {
           "alias": "column_aa88d480135",
           "args": [
            {
             "column": "amount_spent",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 1756896038471,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hotel_name</th><th>total_revenue</th></tr></thead><tbody><tr><td>Taj Falaknuma Palace</td><td>272000.0</td></tr><tr><td>The Ritz-Carlton</td><td>232800.0</td></tr><tr><td>St. Regis</td><td>209000.0</td></tr><tr><td>The Taj Mahal Palace</td><td>188000.0</td></tr><tr><td>Leela Palace</td><td>159000.0</td></tr><tr><td>ITC Maurya</td><td>145000.0</td></tr><tr><td>Park Hyatt</td><td>137000.0</td></tr><tr><td>The Leela Palace</td><td>136000.0</td></tr><tr><td>Four Seasons</td><td>118800.0</td></tr><tr><td>ITC Grand Chola</td><td>113000.0</td></tr><tr><td>Taj Coromandel</td><td>113000.0</td></tr><tr><td>ITC Kohenur</td><td>105000.0</td></tr><tr><td>Taj Palace</td><td>94500.0</td></tr><tr><td>Hyatt Regency</td><td>85000.0</td></tr><tr><td>Trident Nariman Point</td><td>82000.0</td></tr><tr><td>ITC Gardenia</td><td>81000.0</td></tr><tr><td>The Westin Mindspace</td><td>79000.0</td></tr><tr><td>Conrad Bengaluru</td><td>63000.0</td></tr><tr><td>The Oberoi</td><td>50000.0</td></tr><tr><td>Shangri-La</td><td>27000.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Taj Falaknuma Palace",
         272000.0
        ],
        [
         "The Ritz-Carlton",
         232800.0
        ],
        [
         "St. Regis",
         209000.0
        ],
        [
         "The Taj Mahal Palace",
         188000.0
        ],
        [
         "Leela Palace",
         159000.0
        ],
        [
         "ITC Maurya",
         145000.0
        ],
        [
         "Park Hyatt",
         137000.0
        ],
        [
         "The Leela Palace",
         136000.0
        ],
        [
         "Four Seasons",
         118800.0
        ],
        [
         "ITC Grand Chola",
         113000.0
        ],
        [
         "Taj Coromandel",
         113000.0
        ],
        [
         "ITC Kohenur",
         105000.0
        ],
        [
         "Taj Palace",
         94500.0
        ],
        [
         "Hyatt Regency",
         85000.0
        ],
        [
         "Trident Nariman Point",
         82000.0
        ],
        [
         "ITC Gardenia",
         81000.0
        ],
        [
         "The Westin Mindspace",
         79000.0
        ],
        [
         "Conrad Bengaluru",
         63000.0
        ],
        [
         "The Oberoi",
         50000.0
        ],
        [
         "Shangri-La",
         27000.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hotel_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>location</th><th>average_stay_duration</th></tr></thead><tbody><tr><td>Mumbai</td><td>3.84</td></tr><tr><td>Hyderabad</td><td>3.79</td></tr><tr><td>Chennai</td><td>3.62</td></tr><tr><td>Bangalore</td><td>3.26</td></tr><tr><td>Delhi</td><td>3.22</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Mumbai",
         3.84
        ],
        [
         "Hyderabad",
         3.79
        ],
        [
         "Chennai",
         3.62
        ],
        [
         "Bangalore",
         3.26
        ],
        [
         "Delhi",
         3.22
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "average_stay_duration",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hotel_name</th><th>total_bookings</th><th>cancelled_bookings</th><th>cancellation_rate_percent</th></tr></thead><tbody><tr><td>Shangri-La</td><td>5</td><td>3</td><td>60.0</td></tr><tr><td>Conrad Bengaluru</td><td>5</td><td>3</td><td>60.0</td></tr><tr><td>Trident Nariman Point</td><td>5</td><td>3</td><td>60.0</td></tr><tr><td>Hyatt Regency</td><td>5</td><td>2</td><td>40.0</td></tr><tr><td>The Westin Mindspace</td><td>5</td><td>2</td><td>40.0</td></tr><tr><td>ITC Kohenur</td><td>5</td><td>2</td><td>40.0</td></tr><tr><td>ITC Gardenia</td><td>5</td><td>2</td><td>40.0</td></tr><tr><td>Taj Coromandel</td><td>6</td><td>2</td><td>33.33</td></tr><tr><td>The Ritz-Carlton</td><td>8</td><td>1</td><td>12.5</td></tr><tr><td>ITC Maurya</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>The Leela Palace</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>Taj Palace</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>Leela Palace</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>The Oberoi</td><td>3</td><td>0</td><td>0.0</td></tr><tr><td>St. Regis</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>ITC Grand Chola</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>The Taj Mahal Palace</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>Taj Falaknuma Palace</td><td>5</td><td>0</td><td>0.0</td></tr><tr><td>Four Seasons</td><td>4</td><td>0</td><td>0.0</td></tr><tr><td>Park Hyatt</td><td>4</td><td>0</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Shangri-La",
         5,
         3,
         60.0
        ],
        [
         "Conrad Bengaluru",
         5,
         3,
         60.0
        ],
        [
         "Trident Nariman Point",
         5,
         3,
         60.0
        ],
        [
         "Hyatt Regency",
         5,
         2,
         40.0
        ],
        [
         "The Westin Mindspace",
         5,
         2,
         40.0
        ],
        [
         "ITC Kohenur",
         5,
         2,
         40.0
        ],
        [
         "ITC Gardenia",
         5,
         2,
         40.0
        ],
        [
         "Taj Coromandel",
         6,
         2,
         33.33
        ],
        [
         "The Ritz-Carlton",
         8,
         1,
         12.5
        ],
        [
         "ITC Maurya",
         5,
         0,
         0.0
        ],
        [
         "The Leela Palace",
         5,
         0,
         0.0
        ],
        [
         "Taj Palace",
         5,
         0,
         0.0
        ],
        [
         "Leela Palace",
         5,
         0,
         0.0
        ],
        [
         "The Oberoi",
         3,
         0,
         0.0
        ],
        [
         "St. Regis",
         5,
         0,
         0.0
        ],
        [
         "ITC Grand Chola",
         5,
         0,
         0.0
        ],
        [
         "The Taj Mahal Palace",
         5,
         0,
         0.0
        ],
        [
         "Taj Falaknuma Palace",
         5,
         0,
         0.0
        ],
        [
         "Four Seasons",
         4,
         0,
         0.0
        ],
        [
         "Park Hyatt",
         4,
         0,
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hotel_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_bookings",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cancelled_bookings",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "cancellation_rate_percent",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>booking_month</th><th>location</th><th>total_revenue</th></tr></thead><tbody><tr><td>1</td><td>Bangalore</td><td>84000.0</td></tr><tr><td>1</td><td>Chennai</td><td>121000.0</td></tr><tr><td>1</td><td>Delhi</td><td>62000.0</td></tr><tr><td>1</td><td>Hyderabad</td><td>51000.0</td></tr><tr><td>2</td><td>Bangalore</td><td>27000.0</td></tr><tr><td>2</td><td>Hyderabad</td><td>134000.0</td></tr><tr><td>2</td><td>Mumbai</td><td>113000.0</td></tr><tr><td>3</td><td>Bangalore</td><td>61800.0</td></tr><tr><td>3</td><td>Delhi</td><td>29000.0</td></tr><tr><td>3</td><td>Hyderabad</td><td>86000.0</td></tr><tr><td>3</td><td>Mumbai</td><td>93800.0</td></tr><tr><td>4</td><td>Bangalore</td><td>63000.0</td></tr><tr><td>4</td><td>Chennai</td><td>15000.0</td></tr><tr><td>4</td><td>Delhi</td><td>17500.0</td></tr><tr><td>4</td><td>Hyderabad</td><td>76000.0</td></tr><tr><td>4</td><td>Mumbai</td><td>66000.0</td></tr><tr><td>5</td><td>Delhi</td><td>176000.0</td></tr><tr><td>5</td><td>Mumbai</td><td>56000.0</td></tr><tr><td>6</td><td>Chennai</td><td>24000.0</td></tr><tr><td>6</td><td>Delhi</td><td>32000.0</td></tr><tr><td>6</td><td>Hyderabad</td><td>83000.0</td></tr><tr><td>6</td><td>Mumbai</td><td>120000.0</td></tr><tr><td>7</td><td>Bangalore</td><td>48000.0</td></tr><tr><td>7</td><td>Chennai</td><td>100000.0</td></tr><tr><td>7</td><td>Delhi</td><td>66000.0</td></tr><tr><td>7</td><td>Hyderabad</td><td>100000.0</td></tr><tr><td>7</td><td>Mumbai</td><td>58000.0</td></tr><tr><td>8</td><td>Chennai</td><td>135000.0</td></tr><tr><td>8</td><td>Delhi</td><td>18000.0</td></tr><tr><td>8</td><td>Hyderabad</td><td>34000.0</td></tr><tr><td>8</td><td>Mumbai</td><td>91000.0</td></tr><tr><td>9</td><td>Bangalore</td><td>120000.0</td></tr><tr><td>9</td><td>Chennai</td><td>52000.0</td></tr><tr><td>9</td><td>Delhi</td><td>48000.0</td></tr><tr><td>9</td><td>Hyderabad</td><td>29000.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Bangalore",
         84000.0
        ],
        [
         1,
         "Chennai",
         121000.0
        ],
        [
         1,
         "Delhi",
         62000.0
        ],
        [
         1,
         "Hyderabad",
         51000.0
        ],
        [
         2,
         "Bangalore",
         27000.0
        ],
        [
         2,
         "Hyderabad",
         134000.0
        ],
        [
         2,
         "Mumbai",
         113000.0
        ],
        [
         3,
         "Bangalore",
         61800.0
        ],
        [
         3,
         "Delhi",
         29000.0
        ],
        [
         3,
         "Hyderabad",
         86000.0
        ],
        [
         3,
         "Mumbai",
         93800.0
        ],
        [
         4,
         "Bangalore",
         63000.0
        ],
        [
         4,
         "Chennai",
         15000.0
        ],
        [
         4,
         "Delhi",
         17500.0
        ],
        [
         4,
         "Hyderabad",
         76000.0
        ],
        [
         4,
         "Mumbai",
         66000.0
        ],
        [
         5,
         "Delhi",
         176000.0
        ],
        [
         5,
         "Mumbai",
         56000.0
        ],
        [
         6,
         "Chennai",
         24000.0
        ],
        [
         6,
         "Delhi",
         32000.0
        ],
        [
         6,
         "Hyderabad",
         83000.0
        ],
        [
         6,
         "Mumbai",
         120000.0
        ],
        [
         7,
         "Bangalore",
         48000.0
        ],
        [
         7,
         "Chennai",
         100000.0
        ],
        [
         7,
         "Delhi",
         66000.0
        ],
        [
         7,
         "Hyderabad",
         100000.0
        ],
        [
         7,
         "Mumbai",
         58000.0
        ],
        [
         8,
         "Chennai",
         135000.0
        ],
        [
         8,
         "Delhi",
         18000.0
        ],
        [
         8,
         "Hyderabad",
         34000.0
        ],
        [
         8,
         "Mumbai",
         91000.0
        ],
        [
         9,
         "Bangalore",
         120000.0
        ],
        [
         9,
         "Chennai",
         52000.0
        ],
        [
         9,
         "Delhi",
         48000.0
        ],
        [
         9,
         "Hyderabad",
         29000.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "booking_month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5866084564046110>, line 64\u001B[0m\n",
       "\u001B[1;32m     61\u001B[0m output_path_base \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/my_volume/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Save each result DataFrame to its own folder as a single CSV file\u001B[39;00m\n",
       "\u001B[0;32m---> 64\u001B[0m revenue_per_hotel_df\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(output_path_base \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrevenue_per_hotel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     65\u001B[0m avg_stay_by_location_df\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(output_path_base \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg_stay_by_location\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     66\u001B[0m cancellation_rates_df\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(output_path_base \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcancellation_rates\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:807\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n",
       "\u001B[1;32m    788\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n",
       "\u001B[1;32m    789\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m    790\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m    791\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    805\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n",
       "\u001B[1;32m    806\u001B[0m )\n",
       "\u001B[0;32m--> 807\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:679\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    677\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[1;32m    678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n",
       "\u001B[0;32m--> 679\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    681\u001B[0m )\n",
       "\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1481\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1479\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1480\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1481\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1482\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1483\u001B[0m )\n",
       "\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1485\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1970\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1967\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1969\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1970\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1971\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1972\u001B[0m     ):\n",
       "\u001B[1;32m   1973\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1974\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1946\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1944\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1945\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1946\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2266\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2264\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2377\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2363\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2364\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2365\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2373\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n",
       "\u001B[1;32m   2374\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2375\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2377\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2378\u001B[0m                 info,\n",
       "\u001B[1;32m   2379\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2380\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2381\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2382\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2384\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2385\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2386\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2387\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2388\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mUnsupportedOperationException\u001B[0m: Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n",
       "\n",
       "JVM stacktrace:\n",
       "java.lang.UnsupportedOperationException\n",
       "\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n",
       "\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n",
       "\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n",
       "\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n",
       "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n",
       "\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "UnsupportedOperationException",
        "evalue": "Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "metadata": {
        "errorSummary": "Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "XXKCM",
        "stackTrace": "java.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mUnsupportedOperationException\u001B[0m             Traceback (most recent call last)",
        "File \u001B[0;32m<command-5866084564046110>, line 64\u001B[0m\n\u001B[1;32m     61\u001B[0m output_path_base \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/my_volume/\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Save each result DataFrame to its own folder as a single CSV file\u001B[39;00m\n\u001B[0;32m---> 64\u001B[0m revenue_per_hotel_df\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(output_path_base \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrevenue_per_hotel\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     65\u001B[0m avg_stay_by_location_df\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(output_path_base \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mavg_stay_by_location\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     66\u001B[0m cancellation_rates_df\u001B[38;5;241m.\u001B[39mcoalesce(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcsv(output_path_base \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcancellation_rates\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:807\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m    790\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m    791\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    805\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m    806\u001B[0m )\n\u001B[0;32m--> 807\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:679\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    677\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[1;32m    678\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m--> 679\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    681\u001B[0m )\n\u001B[1;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1481\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1479\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1480\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1481\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1482\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1483\u001B[0m )\n\u001B[1;32m   1484\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1485\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1970\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1967\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1969\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1970\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1971\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1972\u001B[0m     ):\n\u001B[1;32m   1973\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1974\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1946\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1944\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1945\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1946\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2266\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2264\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2377\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2363\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2364\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2365\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2373\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n\u001B[1;32m   2374\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2375\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2377\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2378\u001B[0m                 info,\n\u001B[1;32m   2379\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2380\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2381\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2382\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2384\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2385\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2386\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2387\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2388\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mUnsupportedOperationException\u001B[0m: Public DBFS root is disabled. Access is denied on path: /dbfs/tmp/my_volume/revenue_per_hotel/_delta_log\n\nJVM stacktrace:\njava.lang.UnsupportedOperationException\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.rejectOperation(DisabledDatabricksFileSystem.scala:31)\n\tat com.databricks.backend.daemon.data.client.DisabledDatabricksFileSystem.getFileStatus(DisabledDatabricksFileSystem.scala:108)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$2(DatabricksFileSystemV2.scala:1189)\n\tat com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:64)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$getFileStatus$1(DatabricksFileSystemV2.scala:1186)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:742)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:1185)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:211)\n\tat com.databricks.sql.io.LokiFileSystem.getFileStatus(LokiFileSystem.scala:246)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1862)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRootThrowOnError(DeltaTable.scala:367)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:315)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:306)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:298)\n\tat com.databricks.sql.transaction.tahoe.DeltaValidation$.validateNonDeltaWrite(DeltaValidation.scala:203)\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:183)\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:150)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:3755)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3199)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:385)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:282)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:238)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:141)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:53)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:586)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:586)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Import all necessary functions from PySpark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ----------------- LOAD DATA -----------------\n",
    "# IMPORTANT: Replace these paths with your actual file paths from Databricks\n",
    "path_customers = \"/Volumes/workspace/big_data_project/big_data/customers.csv\"\n",
    "path_hotels = \"/Volumes/workspace/big_data_project/big_data/hotels.csv\"\n",
    "path_bookings = \"/Volumes/workspace/big_data_project/big_data/bookings.csv\"\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "customers_df = spark.read.option(\"header\", \"true\").csv(path_customers)\n",
    "hotels_df = spark.read.option(\"header\", \"true\").csv(path_hotels)\n",
    "bookings_df = spark.read.option(\"header\", \"true\").csv(path_bookings)\n",
    "\n",
    "# ----------------- DATA PREPARATION -----------------\n",
    "# Convert columns to the correct data types for calculations\n",
    "bookings_df = bookings_df.withColumn(\"stay_duration\", F.col(\"stay_duration\").cast(\"int\")) \\\n",
    "    .withColumn(\"amount_spent\", F.col(\"amount_spent\").cast(\"double\")) \\\n",
    "    .withColumn(\"booking_date\", F.col(\"booking_date\").cast(\"date\"))\n",
    "\n",
    "# Create a single master DataFrame by joining the tables\n",
    "master_df = bookings_df.join(hotels_df, \"hotel_id\").join(customers_df, \"customer_id\")\n",
    "\n",
    "display(master_df)\n",
    "\n",
    "# ----------------- PYSPARK CORE TASKS -----------------\n",
    "# Task 1: Compute total revenue per hotel (only for 'Booked' status)\n",
    "revenue_per_hotel_df = master_df.filter(F.col(\"status\") == \"Booked\") \\\n",
    "    .groupBy(\"hotel_name\") \\\n",
    "    .agg(F.round(F.sum(\"amount_spent\"), 2).alias(\"total_revenue\")) \\\n",
    "    .orderBy(F.col(\"total_revenue\").desc())\n",
    "display(revenue_per_hotel_df)\n",
    "\n",
    "# Task 2: Calculate average stay duration by location\n",
    "avg_stay_by_location_df = master_df.groupBy(\"location\") \\\n",
    "    .agg(F.round(F.avg(\"stay_duration\"), 2).alias(\"average_stay_duration\")) \\\n",
    "    .orderBy(F.col(\"average_stay_duration\").desc())\n",
    "display(avg_stay_by_location_df)\n",
    "\n",
    "# ----------------- PYSPARK SQL TASKS -----------------\n",
    "# Task 1: Identify hotels with the highest cancellation rates\n",
    "cancellation_rates_df = master_df.groupBy(\"hotel_name\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_bookings\"),\n",
    "        F.sum(F.when(F.col(\"status\") == \"Cancelled\", 1).otherwise(0)).alias(\"cancelled_bookings\")\n",
    "    ) \\\n",
    "    .withColumn(\"cancellation_rate_percent\", F.round((F.col(\"cancelled_bookings\") / F.col(\"total_bookings\")) * 100, 2)) \\\n",
    "    .orderBy(F.col(\"cancellation_rate_percent\").desc())\n",
    "display(cancellation_rates_df)\n",
    "\n",
    "# Task 2: Find seasonal trends (revenue by month and location for 'Booked' status)\n",
    "seasonal_trends_df = master_df.filter(F.col(\"status\") == \"Booked\") \\\n",
    "    .withColumn(\"booking_month\", F.month(\"booking_date\")) \\\n",
    "    .groupBy(\"booking_month\", \"location\") \\\n",
    "    .agg(F.round(F.sum(\"amount_spent\"), 2).alias(\"total_revenue\")) \\\n",
    "    .orderBy(\"booking_month\", \"location\")\n",
    "display(seasonal_trends_df)\n",
    "\n",
    "# ----------------- SAVE OUTPUTS -----------------\n",
    "# Define a base path for all your output files\n",
    "output_path_base = \"/dbfs/tmp/my_volume/\"\n",
    "\n",
    "# Save each result DataFrame to its own folder as a single CSV file\n",
    "revenue_per_hotel_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_path_base + \"revenue_per_hotel\")\n",
    "avg_stay_by_location_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_path_base + \"avg_stay_by_location\")\n",
    "cancellation_rates_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_path_base + \"cancellation_rates\")\n",
    "seasonal_trends_df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(output_path_base + \"seasonal_trends\")\n",
    "\n",
    "print(\"All output files have been successfully saved to your S3 bucket.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f680c92-24ee-4cb7-ac53-a6c7096567bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ae0ceff-f104-4249-b1e3-c42b81d932df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-09-03 15:46:11",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}